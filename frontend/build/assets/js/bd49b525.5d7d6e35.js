"use strict";(globalThis.webpackChunkhumanoid_robotics_textbook=globalThis.webpackChunkhumanoid_robotics_textbook||[]).push([[472],{1937(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"chapter-3/index","title":"Sensors and Actuators","description":"Deep dive into the sensing and actuation systems that enable humanoid robot perception and movement.","source":"@site/docs/chapter-3/index.md","sourceDirName":"chapter-3","slug":"/chapter-3/","permalink":"/Hackathon1-humanoid-robotic-book/chapter-3/","draft":false,"unlisted":false,"editUrl":"https://github.com/Shaistatosif/Hackathon1-humanoid-robotic-book/tree/001-book-generation/frontend/docs/chapter-3/index.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Sensors and Actuators","description":"Deep dive into the sensing and actuation systems that enable humanoid robot perception and movement."},"sidebar":"textbookSidebar","previous":{"title":"Chapter 3: Sensors and Actuators","permalink":"/Hackathon1-humanoid-robotic-book/category/chapter-3-sensors-and-actuators"}}');var r=t(4848),i=t(8453);const o={sidebar_position:3,title:"Sensors and Actuators",description:"Deep dive into the sensing and actuation systems that enable humanoid robot perception and movement."},a="Chapter 3: Sensors and Actuators",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"3.1 Introduction to Robot Sensing",id:"31-introduction-to-robot-sensing",level:2},{value:"Sensor Categories",id:"sensor-categories",level:3},{value:"3.2 Proprioceptive Sensors",id:"32-proprioceptive-sensors",level:2},{value:"Joint Position Encoders",id:"joint-position-encoders",level:3},{value:"Inertial Measurement Units (IMU)",id:"inertial-measurement-units-imu",level:3},{value:"Force/Torque Sensors",id:"forcetorque-sensors",level:3},{value:"3.3 Exteroceptive Sensors",id:"33-exteroceptive-sensors",level:2},{value:"Vision Systems",id:"vision-systems",level:3},{value:"LiDAR (Light Detection and Ranging)",id:"lidar-light-detection-and-ranging",level:3},{value:"3.4 Tactile Sensors",id:"34-tactile-sensors",level:2},{value:"Types of Tactile Sensors",id:"types-of-tactile-sensors",level:3},{value:"3.5 Sensor Fusion",id:"35-sensor-fusion",level:2},{value:"Complementary Filter",id:"complementary-filter",level:3},{value:"Extended Kalman Filter",id:"extended-kalman-filter",level:3},{value:"3.6 Actuator Control",id:"36-actuator-control",level:2},{value:"PID Control",id:"pid-control",level:3},{value:"Summary",id:"summary",level:2},{value:"Review Questions",id:"review-questions",level:2},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2}];function d(e){const n={blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-3-sensors-and-actuators",children:"Chapter 3: Sensors and Actuators"})}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Identify and describe the main sensor types used in humanoid robots"}),"\n",(0,r.jsx)(n.li,{children:"Understand sensor fusion and its importance in robot perception"}),"\n",(0,r.jsx)(n.li,{children:"Explain different actuator control strategies"}),"\n",(0,r.jsx)(n.li,{children:"Design basic sensor processing pipelines"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"31-introduction-to-robot-sensing",children:"3.1 Introduction to Robot Sensing"}),"\n",(0,r.jsx)(n.p,{children:'Sensors are the "eyes and ears" of humanoid robots, providing the information needed to perceive and interact with the environment.'}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Principle:"})," Effective robot sensing requires multiple complementary sensors working together through sensor fusion to create a reliable understanding of the robot's state and environment."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"sensor-categories",children:"Sensor Categories"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from enum import Enum\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\nclass SensorCategory(Enum):\n    """Categories of sensors in humanoid robots."""\n    PROPRIOCEPTIVE = "proprioceptive"  # Internal state\n    EXTEROCEPTIVE = "exteroceptive"    # External environment\n    TACTILE = "tactile"                 # Touch/contact\n\n@dataclass\nclass Sensor:\n    """Base class for robot sensors."""\n    name: str\n    category: SensorCategory\n    sample_rate: float  # Hz\n    resolution: float\n    range_min: float\n    range_max: float\n    noise_std: float\n\n    def is_reading_valid(self, value: float) -> bool:\n        """Check if reading is within sensor range."""\n        return self.range_min <= value <= self.range_max\n'})}),"\n",(0,r.jsx)(n.h2,{id:"32-proprioceptive-sensors",children:"3.2 Proprioceptive Sensors"}),"\n",(0,r.jsx)(n.p,{children:"Proprioceptive sensors measure the internal state of the robot."}),"\n",(0,r.jsx)(n.h3,{id:"joint-position-encoders",children:"Joint Position Encoders"}),"\n",(0,r.jsx)(n.p,{children:"Encoders measure the angular position of joints:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Encoder Type"}),(0,r.jsx)(n.th,{children:"Resolution"}),(0,r.jsx)(n.th,{children:"Advantages"}),(0,r.jsx)(n.th,{children:"Disadvantages"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Incremental"})}),(0,r.jsx)(n.td,{children:"1000-10000 CPR"}),(0,r.jsx)(n.td,{children:"Low cost, simple"}),(0,r.jsx)(n.td,{children:"Requires homing"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Absolute"})}),(0,r.jsx)(n.td,{children:"12-19 bits"}),(0,r.jsx)(n.td,{children:"No homing needed"}),(0,r.jsx)(n.td,{children:"Higher cost"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Magnetic"})}),(0,r.jsx)(n.td,{children:"12-14 bits"}),(0,r.jsx)(n.td,{children:"Robust, compact"}),(0,r.jsx)(n.td,{children:"Lower precision"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Optical"})}),(0,r.jsx)(n.td,{children:"16-23 bits"}),(0,r.jsx)(n.td,{children:"High precision"}),(0,r.jsx)(n.td,{children:"Dust sensitive"})]})]})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import math\n\nclass RotaryEncoder:\n    """Simulates a rotary encoder for joint position sensing."""\n\n    def __init__(self, resolution_bits: int, encoder_type: str = "absolute"):\n        self.resolution_bits = resolution_bits\n        self.counts_per_rev = 2 ** resolution_bits\n        self.encoder_type = encoder_type\n        self._count = 0\n        self._zero_offset = 0\n\n    @property\n    def resolution_deg(self) -> float:\n        """Resolution in degrees per count."""\n        return 360.0 / self.counts_per_rev\n\n    def read_position_deg(self) -> float:\n        """Read current position in degrees."""\n        raw_deg = (self._count / self.counts_per_rev) * 360.0\n        return raw_deg - self._zero_offset\n\n    def read_position_rad(self) -> float:\n        """Read current position in radians."""\n        return math.radians(self.read_position_deg())\n\n    def set_count(self, count: int):\n        """Simulate encoder count (for testing)."""\n        self._count = count % self.counts_per_rev\n\n    def zero(self):\n        """Set current position as zero reference."""\n        self._zero_offset = (self._count / self.counts_per_rev) * 360.0\n\n# Example: 17-bit absolute encoder\nencoder = RotaryEncoder(resolution_bits=17)\nprint(f"Resolution: {encoder.resolution_deg:.6f} degrees")\nprint(f"Counts per revolution: {encoder.counts_per_rev}")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"inertial-measurement-units-imu",children:"Inertial Measurement Units (IMU)"}),"\n",(0,r.jsx)(n.p,{children:"IMUs combine multiple sensors for orientation and motion sensing:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'@dataclass\nclass IMUReading:\n    """Single IMU measurement."""\n    accelerometer: tuple  # (ax, ay, az) in m/s\xb2\n    gyroscope: tuple      # (gx, gy, gz) in rad/s\n    magnetometer: tuple   # (mx, my, mz) in \u03bcT (optional)\n    timestamp: float      # seconds\n\nclass IMU:\n    """Inertial Measurement Unit sensor."""\n\n    def __init__(self, sample_rate: float = 1000):\n        self.sample_rate = sample_rate\n        self.accel_range = 16  # \xb116g\n        self.gyro_range = 2000  # \xb12000 deg/s\n\n        # Typical noise characteristics\n        self.accel_noise = 0.003  # g/\u221aHz\n        self.gyro_noise = 0.01   # deg/s/\u221aHz\n\n        # Bias (drift over time)\n        self.gyro_bias = [0.0, 0.0, 0.0]\n\n    def calibrate_gyro_bias(self, samples: List[IMUReading]):\n        """Calibrate gyroscope bias from stationary samples."""\n        if not samples:\n            return\n\n        avg_gx = sum(s.gyroscope[0] for s in samples) / len(samples)\n        avg_gy = sum(s.gyroscope[1] for s in samples) / len(samples)\n        avg_gz = sum(s.gyroscope[2] for s in samples) / len(samples)\n\n        self.gyro_bias = [avg_gx, avg_gy, avg_gz]\n\n    def remove_bias(self, reading: IMUReading) -> IMUReading:\n        """Remove calibrated bias from gyroscope readings."""\n        corrected_gyro = (\n            reading.gyroscope[0] - self.gyro_bias[0],\n            reading.gyroscope[1] - self.gyro_bias[1],\n            reading.gyroscope[2] - self.gyro_bias[2]\n        )\n        return IMUReading(\n            accelerometer=reading.accelerometer,\n            gyroscope=corrected_gyro,\n            magnetometer=reading.magnetometer,\n            timestamp=reading.timestamp\n        )\n'})}),"\n",(0,r.jsx)(n.h3,{id:"forcetorque-sensors",children:"Force/Torque Sensors"}),"\n",(0,r.jsx)(n.p,{children:"Essential for measuring interaction forces:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class ForceTorqueSensor:\n    """6-axis force/torque sensor for wrist or ankle mounting."""\n\n    def __init__(self, force_range: float = 500, torque_range: float = 50):\n        """\n        Args:\n            force_range: Maximum force in Newtons (Fx, Fy, Fz)\n            torque_range: Maximum torque in Nm (Tx, Ty, Tz)\n        """\n        self.force_range = force_range\n        self.torque_range = torque_range\n        self._calibration_matrix = self._identity_6x6()\n        self._bias = [0.0] * 6\n\n    def _identity_6x6(self):\n        """Create 6x6 identity matrix."""\n        return [[1 if i == j else 0 for j in range(6)] for i in range(6)]\n\n    def read_raw(self) -> List[float]:\n        """Read raw sensor values (simulated)."""\n        # In real implementation, this would read from hardware\n        return [0.0] * 6\n\n    def read_calibrated(self) -> dict:\n        """Read calibrated force/torque values."""\n        raw = self.read_raw()\n\n        # Apply calibration matrix and bias removal\n        calibrated = []\n        for i in range(6):\n            value = sum(self._calibration_matrix[i][j] * raw[j]\n                       for j in range(6))\n            calibrated.append(value - self._bias[i])\n\n        return {\n            "force": {"x": calibrated[0], "y": calibrated[1], "z": calibrated[2]},\n            "torque": {"x": calibrated[3], "y": calibrated[4], "z": calibrated[5]}\n        }\n\n    def tare(self):\n        """Zero the sensor (remove current load as bias)."""\n        raw = self.read_raw()\n        for i in range(6):\n            self._bias[i] = sum(self._calibration_matrix[i][j] * raw[j]\n                               for j in range(6))\n'})}),"\n",(0,r.jsx)(n.h2,{id:"33-exteroceptive-sensors",children:"3.3 Exteroceptive Sensors"}),"\n",(0,r.jsx)(n.p,{children:"Exteroceptive sensors perceive the external environment."}),"\n",(0,r.jsx)(n.h3,{id:"vision-systems",children:"Vision Systems"}),"\n",(0,r.jsx)(n.p,{children:"Cameras are primary sensors for humanoid perception:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'@dataclass\nclass CameraSpec:\n    """Camera sensor specifications."""\n    resolution: tuple    # (width, height)\n    frame_rate: float   # fps\n    field_of_view: tuple  # (horizontal, vertical) degrees\n    focal_length: float  # mm\n    sensor_type: str    # "RGB", "Depth", "RGBD", "Stereo"\n\n    @property\n    def megapixels(self) -> float:\n        """Calculate megapixels."""\n        return (self.resolution[0] * self.resolution[1]) / 1_000_000\n\nclass StereoCamera:\n    """Stereo camera system for depth perception."""\n\n    def __init__(self, baseline: float, focal_length: float,\n                 resolution: tuple):\n        """\n        Args:\n            baseline: Distance between cameras in meters\n            focal_length: Focal length in pixels\n            resolution: (width, height)\n        """\n        self.baseline = baseline\n        self.focal_length = focal_length\n        self.resolution = resolution\n\n    def disparity_to_depth(self, disparity: float) -> float:\n        """\n        Convert disparity (pixel difference) to depth.\n\n        depth = (baseline * focal_length) / disparity\n        """\n        if disparity <= 0:\n            return float(\'inf\')\n        return (self.baseline * self.focal_length) / disparity\n\n    def depth_to_disparity(self, depth: float) -> float:\n        """Convert depth to expected disparity."""\n        if depth <= 0:\n            return float(\'inf\')\n        return (self.baseline * self.focal_length) / depth\n\n    def min_detectable_depth(self, max_disparity: int) -> float:\n        """Minimum depth that can be detected."""\n        return self.disparity_to_depth(max_disparity)\n\n# Example: RealSense D435 style camera\nstereo = StereoCamera(\n    baseline=0.05,        # 50mm baseline\n    focal_length=380,     # pixels\n    resolution=(1280, 720)\n)\n\n# Calculate depth from disparity\ndisparity = 38  # pixels\ndepth = stereo.disparity_to_depth(disparity)\nprint(f"Disparity {disparity}px \u2192 Depth {depth:.2f}m")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"lidar-light-detection-and-ranging",children:"LiDAR (Light Detection and Ranging)"}),"\n",(0,r.jsx)(n.p,{children:"LiDAR provides precise 3D environmental mapping:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'@dataclass\nclass LiDARSpec:\n    """LiDAR sensor specifications."""\n    range_max: float       # meters\n    range_min: float       # meters\n    angular_resolution: float  # degrees\n    scan_rate: float      # Hz\n    channels: int         # number of laser beams\n    field_of_view: tuple  # (horizontal, vertical) degrees\n    accuracy: float       # meters\n\nclass LiDARProcessor:\n    """Process LiDAR point cloud data."""\n\n    def __init__(self, spec: LiDARSpec):\n        self.spec = spec\n\n    def filter_points(self, points: List[tuple],\n                      min_range: float = None,\n                      max_range: float = None) -> List[tuple]:\n        """\n        Filter point cloud by range.\n\n        Args:\n            points: List of (x, y, z) coordinates\n            min_range: Minimum distance (default: sensor min)\n            max_range: Maximum distance (default: sensor max)\n        """\n        min_r = min_range or self.spec.range_min\n        max_r = max_range or self.spec.range_max\n\n        filtered = []\n        for x, y, z in points:\n            distance = (x**2 + y**2 + z**2) ** 0.5\n            if min_r <= distance <= max_r:\n                filtered.append((x, y, z))\n\n        return filtered\n\n    def downsample_voxel(self, points: List[tuple],\n                         voxel_size: float) -> List[tuple]:\n        """\n        Downsample point cloud using voxel grid filter.\n\n        Args:\n            points: List of (x, y, z) coordinates\n            voxel_size: Size of voxel grid cells in meters\n        """\n        voxels = {}\n\n        for x, y, z in points:\n            # Calculate voxel index\n            vx = int(x / voxel_size)\n            vy = int(y / voxel_size)\n            vz = int(z / voxel_size)\n            key = (vx, vy, vz)\n\n            # Keep first point in each voxel\n            if key not in voxels:\n                voxels[key] = (x, y, z)\n\n        return list(voxels.values())\n'})}),"\n",(0,r.jsx)(n.h2,{id:"34-tactile-sensors",children:"3.4 Tactile Sensors"}),"\n",(0,r.jsx)(n.p,{children:'Tactile sensors enable robots to "feel" physical contact.'}),"\n",(0,r.jsx)(n.h3,{id:"types-of-tactile-sensors",children:"Types of Tactile Sensors"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Principle"}),(0,r.jsx)(n.th,{children:"Sensitivity"}),(0,r.jsx)(n.th,{children:"Applications"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Resistive"})}),(0,r.jsx)(n.td,{children:"Pressure changes resistance"}),(0,r.jsx)(n.td,{children:"Medium"}),(0,r.jsx)(n.td,{children:"Grippers"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Capacitive"})}),(0,r.jsx)(n.td,{children:"Capacitance variation"}),(0,r.jsx)(n.td,{children:"High"}),(0,r.jsx)(n.td,{children:"Skin sensors"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Piezoelectric"})}),(0,r.jsx)(n.td,{children:"Generates voltage from pressure"}),(0,r.jsx)(n.td,{children:"Very high"}),(0,r.jsx)(n.td,{children:"Slip detection"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Optical"})}),(0,r.jsx)(n.td,{children:"Light intensity changes"}),(0,r.jsx)(n.td,{children:"High"}),(0,r.jsx)(n.td,{children:"Fingertips"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Barometric"})}),(0,r.jsx)(n.td,{children:"Air pressure in chambers"}),(0,r.jsx)(n.td,{children:"Medium"}),(0,r.jsx)(n.td,{children:"Soft grippers"})]})]})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class TactileSensor:\n    """Tactile sensor array for robot skin."""\n\n    def __init__(self, rows: int, cols: int,\n                 spatial_resolution: float = 0.004):\n        """\n        Args:\n            rows: Number of taxel rows\n            cols: Number of taxel columns\n            spatial_resolution: Distance between taxels in meters\n        """\n        self.rows = rows\n        self.cols = cols\n        self.spatial_resolution = spatial_resolution\n        self.pressure_range = (0, 100)  # kPa\n\n        # Initialize taxel array\n        self.taxels = [[0.0 for _ in range(cols)] for _ in range(rows)]\n\n    @property\n    def total_taxels(self) -> int:\n        """Total number of tactile elements."""\n        return self.rows * self.cols\n\n    @property\n    def sensing_area(self) -> float:\n        """Total sensing area in m\xb2."""\n        width = self.cols * self.spatial_resolution\n        height = self.rows * self.spatial_resolution\n        return width * height\n\n    def read_pressure_map(self) -> List[List[float]]:\n        """Read the full pressure map."""\n        return [row[:] for row in self.taxels]\n\n    def get_contact_centroid(self) -> Optional[tuple]:\n        """Calculate centroid of contact area."""\n        total_pressure = 0\n        cx, cy = 0, 0\n\n        for i, row in enumerate(self.taxels):\n            for j, pressure in enumerate(row):\n                if pressure > 0:\n                    total_pressure += pressure\n                    cx += j * pressure\n                    cy += i * pressure\n\n        if total_pressure == 0:\n            return None\n\n        return (cx / total_pressure, cy / total_pressure)\n\n    def detect_slip(self, previous_map: List[List[float]],\n                    threshold: float = 5.0) -> bool:\n        """\n        Detect potential slip by comparing pressure maps.\n\n        Args:\n            previous_map: Previous pressure reading\n            threshold: Change threshold for slip detection\n        """\n        for i in range(self.rows):\n            for j in range(self.cols):\n                diff = abs(self.taxels[i][j] - previous_map[i][j])\n                if diff > threshold:\n                    return True\n        return False\n\n# Example: Fingertip tactile sensor\nfingertip = TactileSensor(rows=8, cols=8, spatial_resolution=0.003)\nprint(f"Total taxels: {fingertip.total_taxels}")\nprint(f"Sensing area: {fingertip.sensing_area * 1e6:.1f} mm\xb2")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"35-sensor-fusion",children:"3.5 Sensor Fusion"}),"\n",(0,r.jsx)(n.p,{children:"Sensor fusion combines data from multiple sensors for improved accuracy."}),"\n",(0,r.jsx)(n.h3,{id:"complementary-filter",children:"Complementary Filter"}),"\n",(0,r.jsx)(n.p,{children:"Simple fusion of accelerometer and gyroscope for orientation:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import math\n\nclass ComplementaryFilter:\n    """\n    Complementary filter for IMU orientation estimation.\n    Combines gyroscope (fast, drifts) with accelerometer (slow, noisy).\n    """\n\n    def __init__(self, alpha: float = 0.98):\n        """\n        Args:\n            alpha: Weight for gyroscope (0-1). Higher = trust gyro more.\n        """\n        self.alpha = alpha\n        self.pitch = 0.0\n        self.roll = 0.0\n\n    def update(self, accel: tuple, gyro: tuple, dt: float) -> tuple:\n        """\n        Update orientation estimate.\n\n        Args:\n            accel: (ax, ay, az) in m/s\xb2\n            gyro: (gx, gy, gz) in rad/s\n            dt: Time step in seconds\n\n        Returns:\n            (pitch, roll) in radians\n        """\n        ax, ay, az = accel\n        gx, gy, gz = gyro\n\n        # Integrate gyroscope\n        gyro_pitch = self.pitch + gx * dt\n        gyro_roll = self.roll + gy * dt\n\n        # Calculate angles from accelerometer\n        accel_pitch = math.atan2(ay, math.sqrt(ax**2 + az**2))\n        accel_roll = math.atan2(-ax, az)\n\n        # Fuse with complementary filter\n        self.pitch = self.alpha * gyro_pitch + (1 - self.alpha) * accel_pitch\n        self.roll = self.alpha * gyro_roll + (1 - self.alpha) * accel_roll\n\n        return (self.pitch, self.roll)\n\n# Example usage\ncf = ComplementaryFilter(alpha=0.98)\naccel = (0.1, 0.05, 9.8)  # m/s\xb2\ngyro = (0.01, 0.02, 0.0)  # rad/s\npitch, roll = cf.update(accel, gyro, dt=0.001)\nprint(f"Pitch: {math.degrees(pitch):.2f}\xb0, Roll: {math.degrees(roll):.2f}\xb0")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"extended-kalman-filter",children:"Extended Kalman Filter"}),"\n",(0,r.jsx)(n.p,{children:"For more complex state estimation:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class SimpleKalmanFilter:\n    """\n    Simplified 1D Kalman filter for sensor fusion demonstration.\n    """\n\n    def __init__(self, process_noise: float, measurement_noise: float,\n                 initial_estimate: float = 0, initial_uncertainty: float = 1):\n        """\n        Args:\n            process_noise: Process noise variance (Q)\n            measurement_noise: Measurement noise variance (R)\n            initial_estimate: Initial state estimate\n            initial_uncertainty: Initial estimate uncertainty (P)\n        """\n        self.Q = process_noise\n        self.R = measurement_noise\n        self.x = initial_estimate  # State estimate\n        self.P = initial_uncertainty  # Estimate uncertainty\n\n    def predict(self, control_input: float = 0):\n        """\n        Prediction step.\n\n        Args:\n            control_input: Known control input affecting state\n        """\n        # State prediction (simple model: x_new = x + control)\n        self.x = self.x + control_input\n\n        # Uncertainty grows\n        self.P = self.P + self.Q\n\n    def update(self, measurement: float) -> float:\n        """\n        Update step with new measurement.\n\n        Args:\n            measurement: Sensor measurement\n\n        Returns:\n            Updated state estimate\n        """\n        # Kalman gain\n        K = self.P / (self.P + self.R)\n\n        # Update estimate with measurement\n        self.x = self.x + K * (measurement - self.x)\n\n        # Update uncertainty\n        self.P = (1 - K) * self.P\n\n        return self.x\n\n# Example: Fusing two position sensors\nkf = SimpleKalmanFilter(\n    process_noise=0.1,\n    measurement_noise=1.0,\n    initial_estimate=0,\n    initial_uncertainty=10\n)\n\n# Simulate measurements\nmeasurements = [1.2, 0.9, 1.1, 1.0, 0.8, 1.1, 1.0]\nfor m in measurements:\n    kf.predict()\n    estimate = kf.update(m)\n    print(f"Measurement: {m:.2f}, Estimate: {estimate:.2f}, "\n          f"Uncertainty: {kf.P:.3f}")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"36-actuator-control",children:"3.6 Actuator Control"}),"\n",(0,r.jsx)(n.p,{children:"Effective actuator control is essential for smooth, precise movement."}),"\n",(0,r.jsx)(n.h3,{id:"pid-control",children:"PID Control"}),"\n",(0,r.jsx)(n.p,{children:"The most common control strategy:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class PIDController:\n    """PID controller for actuator position/velocity control."""\n\n    def __init__(self, kp: float, ki: float, kd: float,\n                 output_limits: tuple = (-float(\'inf\'), float(\'inf\'))):\n        """\n        Args:\n            kp: Proportional gain\n            ki: Integral gain\n            kd: Derivative gain\n            output_limits: (min, max) output limits\n        """\n        self.kp = kp\n        self.ki = ki\n        self.kd = kd\n        self.output_min, self.output_max = output_limits\n\n        self.integral = 0\n        self.previous_error = 0\n        self.previous_time = None\n\n    def reset(self):\n        """Reset controller state."""\n        self.integral = 0\n        self.previous_error = 0\n        self.previous_time = None\n\n    def compute(self, setpoint: float, measured: float,\n                dt: float) -> float:\n        """\n        Compute control output.\n\n        Args:\n            setpoint: Desired value\n            measured: Current measured value\n            dt: Time step in seconds\n\n        Returns:\n            Control output\n        """\n        error = setpoint - measured\n\n        # Proportional term\n        p_term = self.kp * error\n\n        # Integral term (with anti-windup)\n        self.integral += error * dt\n        i_term = self.ki * self.integral\n\n        # Derivative term\n        if dt > 0:\n            derivative = (error - self.previous_error) / dt\n        else:\n            derivative = 0\n        d_term = self.kd * derivative\n\n        # Calculate output\n        output = p_term + i_term + d_term\n\n        # Apply output limits\n        output = max(self.output_min, min(self.output_max, output))\n\n        # Anti-windup: if output is saturated, don\'t accumulate integral\n        if output == self.output_min or output == self.output_max:\n            self.integral -= error * dt\n\n        self.previous_error = error\n\n        return output\n\n# Example: Joint position controller\njoint_controller = PIDController(\n    kp=100,      # Position gain\n    ki=10,       # Integral gain\n    kd=20,       # Derivative gain\n    output_limits=(-50, 50)  # Torque limits in Nm\n)\n\n# Simulate control loop\nsetpoint = 1.0  # Target position in radians\ncurrent_position = 0.0\ndt = 0.001  # 1 kHz control rate\n\nfor i in range(100):\n    torque = joint_controller.compute(setpoint, current_position, dt)\n    # Simple dynamics simulation\n    acceleration = torque / 1.0  # Assuming unit inertia\n    current_position += acceleration * dt * dt\n\n    if i % 20 == 0:\n        print(f"Step {i}: Position={current_position:.3f}, Torque={torque:.2f}")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"Sensors and actuators are the foundation of humanoid robot interaction with the physical world:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Proprioceptive sensors"})," (encoders, IMUs, F/T sensors) measure internal state"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Exteroceptive sensors"})," (cameras, LiDAR) perceive the environment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tactile sensors"})," enable touch sensing and manipulation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor fusion"})," combines multiple sensors for robust estimation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Control algorithms"})," (PID, model-based) ensure precise actuator behavior"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"What is the difference between proprioceptive and exteroceptive sensors?"}),"\n",(0,r.jsx)(n.li,{children:"Why is sensor fusion necessary in humanoid robots?"}),"\n",(0,r.jsx)(n.li,{children:"Explain how a stereo camera calculates depth from disparity."}),"\n",(0,r.jsx)(n.li,{children:"What are the three terms in a PID controller and their purposes?"}),"\n",(0,r.jsx)(n.li,{children:"How does a tactile sensor array detect slip?"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,r.jsx)(n.p,{children:"Implement a complementary filter that fuses:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Accelerometer data for pitch/roll"}),"\n",(0,r.jsx)(n.li,{children:"Gyroscope data for rate of change"}),"\n",(0,r.jsx)(n.li,{children:"Magnetometer data for yaw"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Test your implementation with simulated sensor data and compare different alpha values."})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>o,x:()=>a});var s=t(6540);const r={},i=s.createContext(r);function o(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);